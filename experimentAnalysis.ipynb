{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *New Evidence of Discrimination Among Local Election Officials*\n",
    "\n",
    "**THIS IS A ONE DAY FILE FOR 2017 APSA** This paper has not been peer reviewed, and so these code are not final\n",
    "\n",
    "**Authors**\n",
    "\n",
    "D. Alex Hughes, University of California, Berkeley\n",
    "Micah Gell-Redman, University of Georgia\n",
    "Natarajan Krishnaswami, University of California, Berkeley\n",
    "Diana Rodenberger, University of California, Berkeley\n",
    "Guillermo Monge, University of California, Berkeley\n",
    "Charles Crabtree, University of Michigan\n",
    "\n",
    "\n",
    "**Analysis Date** \n",
    "August, 2017\n",
    "\n",
    "This file produces the analysis that accompanies the paper \"Who else gets to Vote?\". The analysis in this file contains more than is directly reported in the presented file. As such, it is possible that some of this analysis did not undergo peer review, and should be considered as such. \n",
    "\n",
    "<a id=\"toc\"></a>\n",
    "**Table of Contents** \n",
    "1. [Exploratory Data Analysis](#eda)\n",
    "  1. [Randomization](#randomization) \n",
    "  2. [Response Rate by Experimental Condition](#rbc)\n",
    "  3. [Differences in Baseline Responses, by Geography (Non-Experimental)](#stateDiffs) \n",
    "  4. [Most and Least Responsive States (Non-Experimental)](#mostLeast)\n",
    "\n",
    "2. [Experimental Results](#experimentalResults)\n",
    "  1. [Differences in Response Rates by Ethnic Condition(s)](#experimentalResults) \n",
    "  2. [Estimating Treatment Effects using OLS Models](#ols)\n",
    "  3. [Improving Model Performance Using Blocks](#blocks) \n",
    "  4. [Improving Model Performance Using Blocks and Covariates](#covariates)\n",
    "  5. [Precision Weighted Treatment Effect Estimates](#precisionWeighted)\n",
    "  6. [Plot of Precision Weighted Estimates](#precisionWeightedPlot)\n",
    "\n",
    "3. [Heterogenenous Treatment Effects](#hte)\n",
    "  1. [Racial and Ethnic District Characteristics](#demos)\n",
    "  2. [Political Characteristics](#politics)\n",
    "\n",
    "4. [Interruption by Notification](#interruption)\n",
    "  1. [Rates of Response as function of Time Since Sending](#rateOfResponse)\n",
    "  2. [Time taken for Response](#timeToRespond)\n",
    "  3. [No Difference in Estimated Effect From Email](#noInterruptionDifference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Please cite as: \n",
      "\n",
      " Hlavac, Marek (2015). stargazer: Well-Formatted Regression and Summary Statistics Tables.\n",
      " R package version 5.2. http://CRAN.R-project.org/package=stargazer \n",
      "\n"
     ]
    }
   ],
   "source": [
    "require(data.table, quietly = TRUE)\n",
    "require(sandwich,   quietly = TRUE)\n",
    "require(lfe,        quietly = TRUE)\n",
    "require(stargazer,  quietly = TRUE)\n",
    "require(magrittr,   quietly = TRUE)\n",
    "require(IRdisplay,  quietly = TRUE)\n",
    "require(survival,   quietly = TRUE)\n",
    "require(lubridate,  quietly = TRUE)\n",
    "library(repr,       quietly = TRUE)\n",
    "\n",
    "## set digits to print to screen\n",
    "options(\n",
    "    digits=3, \n",
    "    repr.plot.width=4, \n",
    "    repr.plot.height=4,\n",
    "    jupyter.plot_mimetypes = c(\"text/plain\", \"image/png\" )\n",
    ")\n",
    "\n",
    "rm(list = ls())\n",
    "\n",
    "interrupt = TRUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_html(\"<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "The raw code for this IPython notebook is by default hidden for easier reading.\n",
    "To toggle on/off the raw code, click <a href='javascript:code_toggle()'>here</a>.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions to be used to clean and prepare data for analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load.data <- function(f=\"../mailer/all_data_and_resps.csv\") {\n",
    "    ## just a load function that wraps fread without warnings. \n",
    "    options(warn = -1)\n",
    "    d = fread(f)\n",
    "    d[ , f.blockID := as.factor(blockID)]\n",
    "}\n",
    "\n",
    "rses <- function(mod) {\n",
    "    ## convienience function to pull robust standard errors\n",
    "    ## for reporting. \n",
    "    sqrt(diag(vcovHC(mod)))\n",
    "}\n",
    "\n",
    "felm_rses <- function(mod) { \n",
    "    ## convienience function to pull robust standard errors \n",
    "    ## from a felm object for reporting \n",
    "    mod$STATS$GotResponse$rse\n",
    "}\n",
    "\n",
    "\n",
    "gg.pooled.ate <- function(ate1, ate2, se1, se2) {\n",
    "    ## green and gerber pooled ate\n",
    "    weight1 = (1/se1^2) / (1/se1^2 + 1/se2^2)\n",
    "    weight2 = (1/se2^2) / (1/se1^2 + 1/se2^2)\n",
    "    ## calculate the ate \n",
    "    ate = (weight1 * ate1) + (weight2 * ate2)\n",
    "    return(ate)\n",
    "}\n",
    "\n",
    "gg.pooled.se <- function(se1, se2) {\n",
    "    ## green and gerber pooled standard error \n",
    "    se = sqrt(1 / ( (1/se1^2) + (1/se2^2) ))\n",
    "    return(se)\n",
    "}\n",
    "\n",
    "create.survival.data <- function(cd=censoring.date) {\n",
    "    ## creates a data.table for survival analysis, \n",
    "    ## specifying the censoring data. \n",
    "    ## \n",
    "    ## load data \n",
    "    d=load.data()\n",
    "    ## clean the date field for use as a POSIX class time \n",
    "    d[ , Date := gsub(\"T\", \" \", Date)]\n",
    "    d[ , Date := gsub(\"-07:00\", \" EST\", Date)]\n",
    "    ## create the date fields for non-blank `Date`\n",
    "    d[Date != \"\" , Date.num := as.numeric(as.POSIXct(Date))]\n",
    "    d[ , Date := NULL]\n",
    "    d[ , Date := Date.num]\n",
    "    d[ , Date.num := NULL]\n",
    "    ## create the end of observation time for results that \n",
    "    ## received a reply.\n",
    "    d[ReplyDate != \"\", SurvStop := as.numeric(as.POSIXct(ReplyDate))]\n",
    "    d[GotResponse == 0, SurvStop := as.numeric(as.POSIXct(cd))]\n",
    "    d[ , time := SurvStop - Date]\n",
    "    ## create an outcome that scores whether a row received a response\n",
    "    d[, survRespond := GotResponse]\n",
    "}\n",
    "\n",
    "sem <- function(x) { \n",
    "    sqrt(var(x) / length(x))\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"eda\"></a>\n",
    "# 1. Exploratory Data Analysis \n",
    "[toc](#toc)\n",
    "\n",
    "What percent of individuals -- overall -- responded to our mailer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d <- load.data()\n",
    "\n",
    "d[ , .(\"Response Rate\" = mean(GotResponse))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"randomization\"></a>\n",
    "## 1.A Randomization Check \n",
    "[toc](#toc)\n",
    "\n",
    "We side with Mutz and Pemantle (2011), Humphreys, Sanchez de la Sierra, and van der Windt (2012). In the context of this experiment, assignment to treatment conditions is conducted at random, systematically, with a reasonably amount of data, and as the result of a transparent code base. As such, we have no *ex ante* expectations that randomization should be subject to a chance failure, and rather take the point of Mutz and Pemantle that the realization of our randomization procedure produces an instance of a random process. \n",
    "\n",
    "Nevertheless, in the next cell we report the balance on covariates, by experimental condition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[ , .(\"Mean Density\" = mean(dens_log), \n",
    "       \"SE Density\"   = sem(dens_log), \n",
    "       \"Mean Income\"  = mean(pct_inc2pov_150),\n",
    "       \"SE Income\"    = sem(pct_inc2pov_150),\n",
    "       \"Mean Black\"   = mean(pct_race_black), \n",
    "       \"SE Black\"     = sem(pct_race_black), \n",
    "       \"Mean Latino\"  = mean(pct_latino), \n",
    "       \"SE Latino\"    = sem(pct_latino),\n",
    "       \"Mean Obama\"   = mean(obama_margin), \n",
    "       \"SE Obama\"     = sem(obama_margin), \n",
    "       \"Mean VRA\"     = mean(vra_county_state), \n",
    "       \"SE VRA\"       = sem(vra_county_state)), \n",
    "  keyby = .(ethnic_cue)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"rbc\"></a>\n",
    "## 1.B Response Rate by Experimental Condition \n",
    "[toc](#toc)\n",
    "\n",
    "The primary experimental test in this paper examines whether the response rate of registrars is different, conditional on the racial or ethnic cue that was sent to the registrars. To test this, a simple difference between the response rate (mean of responses per sent message) within experiental groups estimates the ATE. The following two tables report the mean response rate, standard error of that mean, and total number of subjecst in the condition. \n",
    "\n",
    "The first table reports a split between a white name and a racially or ethnically minority name. The second reports a split between white, Latino, Black, and Arab names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_by_minority <- d %>% \n",
    "  .[ , .(meanResponse = mean(GotResponse),\n",
    "         se = sqrt(mean(GotResponse) * (1-mean(GotResponse)) / .N), \n",
    "         n = .N), \n",
    "    keyby = .(ethnic_cue != 0)] %>% \n",
    "  .[ , .(\"Ethnic Cue\" = ethnic_cue, \n",
    "         \"Response Rate\" = meanResponse, \n",
    "         \"Standard Error\" = se, \n",
    "         \"N\" = n)] %>% \n",
    "  .[ , \"Ethnic Cue\" := c(\"White\", \"Minority\")]\n",
    "rr_by_minority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_by_condition <- d %>%\n",
    "  .[ , .(meanResponse = mean(GotResponse),\n",
    "         se = sqrt(mean(GotResponse) * (1-mean(GotResponse)) / .N), \n",
    "         n = .N),\n",
    "    keyby = .(ethnic_cue)] %>% \n",
    "  .[ , .(\"Ethnic Cue\" = ethnic_cue, \n",
    "         \"Response Rate\" = meanResponse, \n",
    "         \"Standard Error\" = se, \n",
    "         \"N\" = n)] %>% \n",
    "  .[ , \"Ethnic Cue\" := c(\"White\", \"Latino\", \"Black\", \"Arab\")]\n",
    "\n",
    "\n",
    "rr_by_condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = capture.output(\n",
    "    stargazer(rr_by_minority, \n",
    "          type     = \"latex\", \n",
    "          out      = \"./table1Minority.tex\",\n",
    "          summary  = FALSE, \n",
    "          title    = \"Response Rate by Minority Condition\", \n",
    "          label    = \"tab:minorityRates\", \n",
    "          float    = FALSE, \n",
    "          table.placement = \"t\", \n",
    "          rownames = TRUE, \n",
    "          header   = FALSE, \n",
    "          flip     = TRUE)\n",
    "    )\n",
    "\n",
    "m = capture.output(\n",
    "    stargazer(rr_by_condition, \n",
    "          type     = \"latex\", \n",
    "          out      = \"./table1Minorities.tex\",\n",
    "          summary  = FALSE, \n",
    "          title    = \"Response Rate by Experimental Condition\", \n",
    "          label    = \"tab:rates\", \n",
    "          float    = FALSE,\n",
    "          table.placement = \"t\",\n",
    "          rownames = TRUE, \n",
    "          header   = FALSE, \n",
    "          flip     = TRUE)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"stateDiffs\"></a>\n",
    "## 1.C Were response rates meaningfully different in different parts of the country? \n",
    "[toc](#toc)\n",
    "\n",
    "\n",
    "While we are performing exploratory data analyis, we examine a number of *NON EXPERIMENTAL*: checks to see if things operate differently in different types of districts. \n",
    "\n",
    "Article I: Section 4 of the US Constitution provides that states shall determine the method of electing their representation. These determinations have led to considerable variation. But, has this variation had a meaningful impact on how the Registrars in different states respond to our stimulus? \n",
    "\n",
    "We first note that there is considerable variation in the magnitude of registrars per state; much more than other institutional features (e.g. Members of Congress)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d %>% \n",
    "  .[ , .(numberOfRegistrars = .N), keyby = .(state)] %>% \n",
    "  .[order(numberOfRegistrars, decreasing = TRUE)] %>%\n",
    "  .[1:10 , .(\"State\" = state, \"Number of Registrars\" = numberOfRegistrars)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the distribution of the response rate of these registrars? What is the distribution of responses on a state-by-state basis? In the next two histograms we report these statistics. The first histogram reports the response rate per state. The second reports the total number of responses per state (on a logged scale, due to the *Wisconsin* and *Michigan* having an outsized number of registrars). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateResponses <- d %>% \n",
    "  .[ , .(meanResponseRate = mean(GotResponse), \n",
    "         numRespond = sum(GotResponse)), \n",
    "    keyby = .(state)]\n",
    "\n",
    "pdf(\"./responseHistogram.pdf\", height = 5, width = 10)\n",
    "par(mfrow = c(1,2))\n",
    "source(\"http://ischool.berkeley.edu/~d.alex.hughes/code/pubPlot.R\")\n",
    "hist(stateResponses[ , meanResponseRate], \n",
    "     col = \"black\", xlab = \"Response Rate\",  \n",
    "     main = \"Response Rate per State\")\n",
    "hist(stateResponses[ , log(numRespond)],   \n",
    "     col = \"black\", xlab = \"Log(Responses)\", \n",
    "     main = \"Responses per State\")\n",
    "dev.off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./responseHistogram.pdf\" alt=\"Experiment Overview\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"mostLeast\"></a>\n",
    "## 1.D What states were the most responsive? Which were the least responsive? \n",
    "[toc](#toc)\n",
    "\n",
    "In the following tables, we report the 8 most responsive states and the 8 least responsive states, as well as the total number of responses in each state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateResponses %>% \n",
    "  .[order(meanResponseRate, decreasing = TRUE), ] %>% \n",
    "  .[1:8, ] %>% \n",
    "  .[ , .(\"State\" = state, \n",
    "         \"Response Rate\" = meanResponseRate, \n",
    "         \"Number of Responses\" = numRespond)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What states were the *least* responsive? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateResponses %>% \n",
    "  .[order(meanResponseRate, decreasing = FALSE), ] %>%\n",
    "  .[1:15, ] %>% \n",
    "  .[ , .(\"State\" = state, \n",
    "         \"Response Rate\" = meanResponseRate, \n",
    "         \"Number of Responses\" = numRespond)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is considerable difference in the responsiveness of states. In a number of states nearly 90% of the emails sent are replied to; in other states, fewer than 50% of emails are responded to. \n",
    "\n",
    "One of the primary heterogeneities that we have proposed to examine in this work is whether Registrars in states previously covered by VRA Section 5 protections react differently to experimental stimulus than Registrars in states previously not covered by VRA Section 5 protections. At this point, and throughout, it is important to reiterate that this form of comparison is non-experimental. Any differences in observed responsiveness across a non-experimentally assigning split *may* or *may not* be causally associated with the splitting factor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"baselineDiff\"></a>\n",
    "## Baseline Differences in Responsiveness \n",
    "To set the expectation for later heterogeneous treatment effects that we will examine, here we report the **non experimental** differences in the response rate per state. To do so, we estimate differences in the response rate based on:\n",
    "\n",
    "1. Past VRA status\n",
    "2. Current Latino population\n",
    "3. Current Black population\n",
    "\n",
    "We note that it is not possible to make a similar split based on Arab residents. This information is not currently collected by the US Census Bureau. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we report this split based on whether a registrar worked in a district that had previously been covered by Section 5 of the VRA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d %>% \n",
    "  .[ , .(responseRate = mean(GotResponse)), \n",
    "    keyby = .(\"Previously Covered by VRA\" = vra_county_state)] %>%\n",
    "  .[ , .(\"Previously Covered by VRA\" = c(\"No\", \"Yes\"), \n",
    "         \"Response Rate\" = responseRate) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we report this split based on whether a registrars worked in a district with a Latino population larger than the median Latino population for all the districts in our sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d %>% \n",
    "  .[ , .(responseRate = mean(GotResponse)), \n",
    "    keyby = .(pct_latino > median(pct_latino, na.rm = TRUE))] %>% \n",
    "  .[ , .(\"> Median Latino Population\" = pct_latino, \n",
    "         \"Response Rate\" = responseRate)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third, we report this split based on whether a registrar worked in a district with a Black population larger than the median Black population for all the districts in our sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d %>% \n",
    "  .[ , .(responseRate = mean(GotResponse)), \n",
    "    keyby = .(pct_race_black > median(pct_race_black, na.rm = TRUE))] %>% \n",
    "  .[ , .(\"> Median Black Population\" = pct_race_black, \n",
    "         \"Response Rate\" = responseRate)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"experimentalResults\"></a>\n",
    "# 2. Experimental Results \n",
    "## 2.A Differences in Response Rates by Ethnic Condition(s) \n",
    "[toc](#toc)\n",
    "\n",
    "\n",
    "\n",
    "Here, we report the primary findings of this paper. \n",
    "\n",
    "First, when a question about what documents are necessary to provide in order to vote is asked by a minority identity (*not* a white identity) Registrars are less likely to respond to that query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d %>% \n",
    "  .[ , .(meanResponse = mean(GotResponse), \n",
    "         se = sqrt(var(GotResponse) / .N)), \n",
    "    keyby = .(\"Ethnic Cue\" = ethnic_cue != 0)] %>%\n",
    "  .[ , .(\"Ethnic Cue\" = c(\"White\", \"Minority\"), \n",
    "         \"Response Rate\" = meanResponse, \n",
    "         \"Standard Error\" = se)]\n",
    "\n",
    "display_html(\"<b> Causal effect of contact by a minority sender: </b>\")\n",
    "d %>% \n",
    "  .[ , .(meanResponse = mean(GotResponse)), \n",
    "    keyby = .(I(ethnic_cue!=0))] %>%\n",
    "  .[ , round(diff(meanResponse) * 100, 4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, these differences depend on the ethnic condition that is provided to the Registrar. There is no evidence to suggest that black and white identities are responded to at different rates. Indeed, black and white mailers are responded to at nearly identical rates. However, Latino identities are responded to at about 3% lower rates than white identities; and, Arab identities are responded to at about 10% lower rates than white identities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d %>% \n",
    "  .[ , .(meanResponse = mean(GotResponse), \n",
    "         se = sqrt(var(GotResponse) / .N)), \n",
    "    keyby = .(\"Ethnic Cue\" = ethnic_cue)] %>% \n",
    "  .[ , .(\"Ethnic Cue\" = c(\"White\", \"Latino\", \"Black\", \"Arab\"), \n",
    "         \"Mean Response\" = meanResponse, \n",
    "         \"Standard Error\" = se)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nonparametric Wilcoxon Rank-Sum Tests for the difference in location of ranked means, as well as t-tests for differences in means provide evidence in support of lower rates of response to Latino and Arab names (vis-a-vis white names), but do not provide such support for Black names.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wilcox_minority = d[ , wilcox.test(GotResponse*1 ~ I(ethnic_cue != 0))]\n",
    "wilcox_latino   = d[ethnic_cue %in% c(0,1), wilcox.test(GotResponse*1 ~ I(ethnic_cue == 1))]\n",
    "wilcox_black    = d[ethnic_cue %in% c(0,2), wilcox.test(GotResponse*1 ~ I(ethnic_cue == 2))]\n",
    "wilcox_arab     = d[ethnic_cue %in% c(0,3), wilcox.test(GotResponse*1 ~ I(ethnic_cue == 3))]\n",
    "\n",
    "comparison_table = data.frame( \n",
    "  Test = c(\"All Minorities v. White\", \"Latino v. White\",\n",
    "           \"Black v. White\", \"Arab v. White\"), \n",
    "  P.Value = round(\n",
    "      c(wilcox_minority$p.value, wilcox_latino$p.value, \n",
    "              wilcox_black$p.value, wilcox_arab$p.value), 3)\n",
    "    )\n",
    "comparison_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ols\"></a>\n",
    "## 2.B Estimating Treatment Effects using OLS Models \n",
    "[toc](#toc)\n",
    "\n",
    "OLS estimators -- even when estimated on dichotomous data -- are BLUE estimators of causal effects and are familiar to political scientists (Wooldridge, Angrist & Pischke). \n",
    "\n",
    "In this sub-section, we estimate models that provide the same information as provided in the difference-in-means estimators of the previous sub-section. \n",
    "\n",
    "For each model, we estimate regression coefficients using OLS and then estimate Heteroskedstic Consistent Standard Errors. Rather than assuming normality of the residuals of these models (an assumption certainly violated with dichotomous outcome data) we utilize the White robust SE estimate. In the analysis reported in the supplemental information, we note that changing the link function does not meaningfully change the point estimate or interpretation of model estimates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "null_mod       <- d[ , lm(GotResponse ~ 1)] \n",
    "minority_mod   <- d[ , lm(GotResponse ~ 1 + I(ethnic_cue != 0))]\n",
    "minorities_mod <- d[ , lm(GotResponse ~ 1 + factor(ethnic_cue))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anova_minority   <- anova(null_mod, minority_mod, test = \"F\")\n",
    "anova_minorities <- anova(null_mod, minorities_mod, test = \"F\")\n",
    "\n",
    "comparison_table <- data.frame( \n",
    "  F.Test = c(\"Non-White v. White\", \"Minority Classes v. White\"), \n",
    "  P.Value = round(\n",
    "    c(anova_minority[2,6], anova_minorities[2,6]), 3)\n",
    ")\n",
    "comparison_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 1: OLS Treatment Effects \n",
    "We first estimate treatment effects using the OLS estimator and heteroskedastic-consistent (robust) standard errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stargazer(minority_mod, minorities_mod, \n",
    "          type = 'text',\n",
    "          covariate.labels = c(\"Minority\", \"Latino\", \"Black\", \"Arab\"), \n",
    "          se = list(rses(minority_mod), rses(minorities_mod)), \n",
    "          omit.stat = c(\"F\", \"ser\")\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"blocks\"></a>\n",
    "## 2.C Improve Model Performance Using Blocks \n",
    "[toc](#toc)\n",
    "\n",
    "To improve model performance, we estimate a within estimator using the blocks created before treatment was assigned. Because there are a large number of blocks, we utilize the `felm` call from the package `lfe` which uses a method of projection to remove the mean response rates within each block. Estimates from this method are identical to estimates that simply include block fixed effects. The treatment effect estimates using blocks are highly similar to the treatment effect estimates without blocks -- which is to be expected because treatment assignment is intentionally orthagonal to block -- though model efficiency is higher due to the decreases residual varaince in the model. \n",
    "\n",
    "Unblocked models are estimated as OLS regressions of the following equations: \n",
    "\n",
    "\\begin{align} \n",
    "Y_{i} & = \\beta_{0} + \\beta_{1} Minority + \\epsilon_{i} \\\\ \n",
    "Y_{i} & = \\beta_{0} + \\beta_{1} Latino + \\beta_{2} Black + \\beta_{3} Arab + \\epsilon_{i} \\\\ \n",
    "\\end{align}\n",
    "\n",
    "Blocked models are estimated as FELM regressions of the following *within-estimator* equations: \n",
    "\n",
    "\\begin{align} \n",
    "Y_{i} &= \\beta_{1} Minority + \\phi BlockID + \\epsilon_{i} \\\\\n",
    "Y_{i} &= \\beta_{1} Latino + \\beta_{2} Black + \\beta_{3} Arab + \\phi BlockID + \\epsilon_{i}\n",
    "\\end{align} \n",
    "\n",
    "Notably, with block-level fixed effects there is no sense in estimating an intercept, as the intercept is simply the omitted block. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minority_mod_felm   <- d[ , felm(GotResponse ~ 1 + I(ethnic_cue != 0) | factor(blockID))]\n",
    "minorities_mod_felm <- d[ , felm(GotResponse ~ 1 + factor(ethnic_cue) | factor(blockID))]\n",
    "\n",
    "stargazer(minority_mod, minorities_mod, minority_mod_felm, minorities_mod_felm, \n",
    "          type = \"text\",  \n",
    "          se = list(rses(minority_mod), \n",
    "                    rses(minorities_mod), \n",
    "                    felm_rses(minority_mod_felm), \n",
    "                    felm_rses(minorities_mod_felm)),\n",
    "          covariate.labels = c(\"Minority\", \"Latino\", \"Black\", \"Arab\"), \n",
    "          add.lines = list(c(\"Block FE\", \"No\", \"No\", \"Yes\", \"Yes\")), \n",
    "          omit.stat = c(\"F\", \"ser\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Briefly, we interpret these model coefficients in the following way: \n",
    "\n",
    "- Estimates of the causal effect of sending contact to registrars from white or minority sender names is unaffected by the decicision to use blocking in the estimating equation. \n",
    "- As expected, estimating these causal effects with blocks leads to small improvmeents in model efficiency.\n",
    "- Compared to being contacted by a white voter, being contacted by a minority voter causes a decrease in the response rate of 4.7 percent $(P < 0.0005)$. \n",
    "- Compared to being contacted by a white voter, being contacted by a Latino voter causes a decrease in the response rate of 3.0 percent $(P = 0.069)$.\n",
    "- Compared to being contacted by a white voter, being contacted by a Black voter does not change the response rate. \n",
    "- Compared to being contacted by a white voter, being contacted by an Arab voter causes a decrease in the response rate of 11.3 percent $(P < 6*10^-12)$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"covariates\"></a>\n",
    "## 2.D Increased Precision with Blocks and Covariates \n",
    "[toc](#toc)\n",
    "\n",
    "One possobility is that we can further improve our estimates by using not only the block indicators but also the underlying data features that were used to create the blocks. In practice, however, including these features does not seem to have a demonstrable effect on the estimates. \n",
    "\n",
    "In these models, we estimate the following equations: \n",
    "\n",
    "\\begin{align} \n",
    "Y_{i} = & \\beta_{1} Minority + \\gamma_{1}Population\\_Density + \\gamma_{2}SES\\_Indicator + \\gamma_{3} Pct\\_Black + \\gamma_{4}Pct\\_Latino + \\gamma_{5} Obama\\_Margin\\_2012 + \\phi BlockID + \\epsilon_{i} \\\\ \n",
    "Y_{i} = & \\beta_{1} Latino + \\beta_{2} Black + \\beta_{3} Arab + \\gamma_{1}Population\\_Density + \\gamma_{2}SES\\_Indicator + \\gamma_{3} Pct\\_Black + \\gamma_{4}Pct\\_Latino + \\gamma_{5} Obama\\_Margin\\_2012 + \\phi BlockID + \\epsilon_{i} \\\\ \n",
    "\\end{align} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minority_mod2 <- d[ , felm(GotResponse ~ I(ethnic_cue != 0) + dens_log + \n",
    "                           pct_inc2pov_150 + pct_race_black + pct_latino + \n",
    "                           obama_margin | factor(blockID)) ]\n",
    "minorities_mod2 <- d[ , felm(GotResponse ~ factor(ethnic_cue) + dens_log + \n",
    "                             pct_inc2pov_150 + pct_race_black + pct_latino + \n",
    "                             obama_margin| factor(blockID)) ]\n",
    "\n",
    "stargazer(minority_mod2, minorities_mod2, \n",
    "          type = \"text\", \n",
    "          se = list(felm_rses(minority_mod2), \n",
    "                    felm_rses(minorities_mod2)), \n",
    "          covariate.labels = c(\"Minority\", \"Latino\", \"Black\", \"Arab\", \n",
    "                               \"Pop. Density\", \"Poverty Rate\", \"Percent Pop. Black\", \n",
    "                               \"Percent Pop. Latino\", \"2012 Obama Margin\") \n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"precisionWeighted\"></a>\n",
    "## 2.E Precision-Weighted Treatment Effect Estimates \n",
    "[toc](#toc)\n",
    "\n",
    "In this section we reproduce the main result from White, Nathan and Faller (2015): The difference in the rates of reply to Latino purpotedly registered voters viz white registered voters. To do so, we load the White, Nathan and Faller replication data, execute a limited amount of the replication code, and store results in a model object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load('../whiteNathanFaller_replicationFiles/voterIDexp_data_sept2014.Rdata')\n",
    "\n",
    "rep.d = data.table(data)\n",
    "rm(data)\n",
    "\n",
    "## clean data \n",
    "cat('cleaning data as do White, Nathan and Faller. \\n')\n",
    "cat('note bad data.table ethos to suppress printing for jupyter notebook. \\n')\n",
    "rep.d <- rep.d[ , abs_acc := 0]\n",
    "rep.d <- rep.d[accuracy==\"Absolutely accurate\", abs_acc := 1]\n",
    "rep.d <- rep.d[is.na(response)==TRUE|response==0, abs_acc := NA]\n",
    "rep.d <- rep.d[ , non_info := 0]\n",
    "rep.d <- rep.d[accuracy==\"Non-informative response\", non_info := 1]\n",
    "rep.d <- rep.d[is.na(response)|response==0, non_info := NA]\n",
    "\n",
    "## split to just the first mailing (reported in main text of White, Nathan,\n",
    "## Faller 2015\n",
    "week1 <- rep.d[week==1]\n",
    "week1 <- week1[code !=41 & code!=12]\n",
    "week1 <- week1[ , code.fac := as.factor(as.character(code))]\n",
    "\n",
    "## split to form and 'id' dataset and a 'control' dataset \n",
    "week1id <- week1[week1$first_text==1]\n",
    "week1pr <- week1[week1$first_text==0]\n",
    "\n",
    "display_html(\"<b> WNF, Overall Response Rate: </b>\")\n",
    "week1id[ , .(\"Response Rate\" = mean(response, na.rm = T))]\n",
    "\n",
    "display_html(\"<b> WNF, Response Rate by Condition: </b>\")\n",
    "week1id[ , .(meanResponse = mean(response, na.rm = T)), \n",
    "       keyby = .(\"Minority\" = first_name_latino)] %>% \n",
    "  .[ , .(\"Minority\"  = c(\"No\", \"Yes\"), \n",
    "         \"Response Rate\" = meanResponse)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Green and Gerber ATE and SE(ATE) \n",
    "In the following cell, we calculate the average treatment effect and standard error of that average treatment effect as presented in Green and Gerbers' _Field Experiemnts_. Additionally, we use fixed-effects meta analysis, to combine the results of our study with the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calcuate wnf latino effect\n",
    "diff.wnf = week1id[ , .(mean.response = mean(response, na.rm=T)),\n",
    "    keyby = .(first_name_latino)][ , diff(mean.response)]\n",
    "se.wnf = week1id[ , .(var.part = var(response, na.rm=T), n = .N),\n",
    "    keyby = .(first_name_latino)][ ,\n",
    "        .(gg.se = sqrt(sum(var.part / n)))]\n",
    "confint.wnf = diff.wnf + qt(p=c(.025, .975), df = week1id[ , .N] - 4) %o% se.wnf\n",
    "\n",
    "## calculate the 2016 latino effect \n",
    "diff.h.2016 = d[ethnic_cue %in% 0:1, .(mean.response = mean(GotResponse, na.rm = T)),\n",
    "    keyby = .(ethnic_cue == 1)][ , diff(mean.response)]\n",
    "se.h.2016 = d[ethnic_cue %in% 0:1, .(var.part = var(GotResponse, na.rm=T), n = .N),\n",
    "    keyby = .(ethnic_cue == 1)][ ,\n",
    "                  .(gg.se = sqrt(sum(var.part / n)))]\n",
    "confint.h.2016 = diff.h.2016 +\n",
    "    qt(p=c(.025, .975), df=d[ethnic_cue %in% 0:1, .N] - 4) %o% se.h.2016\n",
    "\n",
    "## calculate effect of all minoritites vs. white in 2016 experiment\n",
    "diff.2016 = d[ , .(mean.response = mean(GotResponse, na.rm = T)),\n",
    "    keyby = .(ethnic_cue != 0)][ , diff(mean.response)]\n",
    "se.2016 = d[ , .(var.part = var(GotResponse, na.rm=T), n = .N),\n",
    "    keyby = .(ethnic_cue != 0)][ ,\n",
    "                  .(gg.se = sqrt(sum(var.part / n)))]\n",
    "confint.2016 = diff.2016 +\n",
    "    qt(p=c(.025, .975), df=d[, .N] - 7) %o% se.2016\n",
    "\n",
    "## calculate the effect for each minority group vs. white in 2016 experiment\n",
    "## double check this!! \n",
    "diffs.2016 = d[ , .(mean.response = mean(GotResponse, na.rm = T), n = .N),\n",
    "    keyby = .(ethnic_cue)][ , .(mean.response = mean.response,\n",
    "        effect = mean.response - mean.response[1], n = n)\n",
    "        ]\n",
    "ses.2016 = d[ , .(var.part = var(GotResponse, na.rm=T), n = .N),\n",
    "    keyby = .(ethnic_cue)][ ,\n",
    "        .(gg.se = sqrt(sum(var.part[1] / n[1], var.part / n))), keyby = .(ethnic_cue)\n",
    "        ]\n",
    "diffs.2016 = cbind(diffs.2016, ses.2016)\n",
    "\n",
    "diffs.2016[ , ':='(low.ci  = effect - qt(0.025, df = n[1] + n-4) * gg.se, \n",
    "                   high.ci = effect + qt(0.025, df = n[1] + n-4) * gg.se) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the precision weighted ATE? \n",
    "In the following cells, we compute the precision weighted ATE drawn from *FEDAI*. In particular, we estimate that the pooled ATE for the prior and current experiments, $\\hat{ATE}_{pooled}$ is: \n",
    "\n",
    "\\begin{equation} \n",
    "    \\hat{ATE}_{pooled} = \\frac{\\frac{1}{\\hat{\\sigma}^{2}_{1}}}{\\frac{1}{\\hat{\\sigma}^{2}_{1}} + \\frac{1}{\\hat{\\sigma}^{2}_{2}}} \\hat{ATE}_{1} + \\frac{\\frac{1}{\\hat{\\sigma}^{2}_{2}}}{\\frac{1}{\\hat{\\sigma}^{2}_{1}} + \\frac{1}{\\hat{\\sigma}^{2}_{2}}} \\hat{ATE}_{2}\n",
    "\\end{equation} \n",
    "\n",
    "and the standard error for this pooled estimator is: \n",
    "\n",
    "\\begin{equation} \n",
    "    \\sqrt{Var(\\hat{ATE}_{pooled})} = \\left[\\frac{1}{\\hat{\\sigma}^{2}_{1}} + \\frac{1}{\\hat{\\sigma}^{2}_{2}}\\right]^{-0.5}\n",
    "\\end{equation} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pw.ate = gg.pooled.ate(ate1=diff.wnf, ate2=diff.h.2016, se1=se.wnf, se2=se.h.2016)\n",
    "pw.se = gg.pooled.se(se1=se.wnf, se2=se.h.2016)\n",
    "pw.ci = as.numeric(pw.ate) + qt(c(.025, .975), sum(d[ethnic_cue %in% 0:1, .N], week1[, .N])) %o% as.numeric(pw.se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sprintf(\"The precision weighted ATE is: %.2f percent.\", pw.ate*100)\n",
    "sprintf(\"The precision weighted SE is: %.2f percent.\", pw.se * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"precisionWeightedPlot\"></a>\n",
    "## Plot these Effects \n",
    "[toc](#toc)\n",
    "\n",
    "In this section, we create the data objects used in the plot in *Figure 1* in the main text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot.obj = data.table(\n",
    "    model = c(\"WNF\", \"2016 Minority\", \"2016 Latino\", \n",
    "              \"2016 Black\", \"2016 Arab\", \"Precision Weighted\"), \n",
    "    effect = NA, se = NA, \n",
    "    low.ci = NA, high.ci = NA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.obj[ , ':='(effect = c(diff.wnf, diff.2016, diffs.2016[-1, effect], pw.ate), \n",
    "                 se = c(se.wnf, se.2016, diffs.2016[-1, gg.se], pw.se), \n",
    "                 low.ci = c(confint.wnf[1], confint.2016[1], diffs.2016[-1, low.ci], pw.ci[1]),\n",
    "                 high.ci = c(confint.wnf[2], confint.2016[2], diffs.2016[-1, high.ci], pw.ci[2]))]\n",
    "plot.obj = plot.obj[c(1, 3, 6, 2,4,5)]\n",
    "plot.obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf(file = './fig1.pdf', width = 14, height = 5)\n",
    "\n",
    "par(lwd = 2,             cex.main = 2,\n",
    "    font.lab = 2,        cex.lab = 2,\n",
    "    font.lab = 2,        cex.axis = 1.5,\n",
    "    font.axis = 2,       font = 2,\n",
    "    col.axis = \"gray50\", las = 1,\n",
    "    mar = c(5.1, 5.1, 4.1, 0.5),\n",
    "    bty = \"n\",           pch = 19\n",
    "    )\n",
    "\n",
    "layout(mat=matrix(1:2, byrow=T, nrow=1, ncol=2),\n",
    "       widths=c(1,1),\n",
    "       heights=c(1,1)\n",
    "       )\n",
    "\n",
    "plot.obj[model %in% c(\"2016 Latino\", \"2016 Black\", \"2016 Arab\") , \n",
    "         plot(x = 1:.N, y = effect, ylim = c(-.15, .05), xlim = c(0.5, 3.5), \n",
    "              xaxt = 'n', xlab = NA, yaxt = 'n', ylab = NA, \n",
    "              main = \"2016 Discrimination\", \n",
    "              cex = 2)]\n",
    "axis(1, at = 1:3, labels = c(\"Latino\", \"Black\", \"Arab\"), lwd = 0)\n",
    "axis(2, at = seq(-.15, .05, by = .05), labels = paste0(seq(-15,5,5), \"%\"))\n",
    "text(x = .75, y = 0.04, labels = \"(a)\", cex = 2, col = \"Grey 50\")\n",
    "abline(h=0, col = \"grey\", lty = 2)\n",
    "for(i in 1:3) { \n",
    "    plot.obj[model %in% c(\"2016 Latino\", \"2016 Black\", \"2016 Arab\")[i], \n",
    "             arrows(x0=i, x1=i, y0 = low.ci, y1 = high.ci, code = 0, lwd = 4)]\n",
    "}\n",
    "for(i in 1:3) { \n",
    "    plot.obj[model %in% c(\"2016 Latino\", \"2016 Black\", \"2016 Arab\")[i], \n",
    "             arrows(x0=i, x1=i, y0 = as.numeric(effect)-as.numeric(se), y1 = as.numeric(effect)+as.numeric(se), code = 0, lwd = 8)]\n",
    "}\n",
    "\n",
    "plot.obj[model %in% c(\"WNF\", \"2016 Latino\", \"Precision Weighted\") , \n",
    "         plot(x = 1:.N, y = effect, ylim = c(-.15, .05), xlim = c(0.5, 3.5), \n",
    "              xaxt = 'n', xlab = NA, yaxt = 'n', ylab = NA, \n",
    "              main = \"2012 & 2016 Discrimination\", \n",
    "              cex = 2)]\n",
    "axis(1, at = 1:3, labels = c(\"2012 Latino\", \"2016 Latino\", \"Weighted \\nAverage\"), lwd = 0)\n",
    "axis(2, at = seq(-.15, .05, by = .05), labels = paste0(seq(-15,5,5), \"%\"))\n",
    "abline(h=0, col = \"grey\", lty = 2)\n",
    "text(x = .75, y = 0.04, labels = \"(b)\", cex = 2, col = \"Grey 50\")\n",
    "for(i in 1:3) { \n",
    "    plot.obj[model %in% c(\"WNF\", \"2016 Latino\", \"Precision Weighted\")[i], \n",
    "             arrows(x0=i, x1=i, y0 = low.ci, y1 = high.ci, code = 0, lwd = 4)]\n",
    "}\n",
    "for(i in 1:3) { \n",
    "    plot.obj[model %in% c(\"WNF\", \"2016 Latino\", \"Precision Weighted\")[i], \n",
    "             arrows(x0=i, x1=i, y0 = as.numeric(effect)-as.numeric(se), y1 = as.numeric(effect)+as.numeric(se), code = 0, lwd = 8)]\n",
    "}\n",
    "\n",
    "dev.off()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./fig1.pdf\" alt=\"Experiment Overview\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"hte\"></a>\n",
    "# 3. Heterogeneous Treatment Effects \n",
    "\n",
    "<a id=\"demos\"></a>\n",
    "## 3.A Demographic Characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, we do not build this data using the the data intake pipeline. This allows for matching at the county and sub-county level, as was performed for all other variables. We note, here, that there is one strange problem that we are working to identify: there are 33 registrars that don't match back onto the original dataset. @diana is doing the QA to see what is going on. Once those are found, we intend to move this late-stage merge into the data-intake pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d.arab <- fread(\"../data/final_email_list_enriched.csv\")\n",
    "d = merge(d, d.arab[ , .(email, pct_arab)], by.x = \"registrar_email\", by.y = \"email\", all.x = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hte.black.null = d[ , lm(GotResponse ~ I(ethnic_cue != 0) + pct_race_black)]\n",
    "hte.black      = d[ , lm(GotResponse ~ I(ethnic_cue != 0) * pct_race_black)]\n",
    "\n",
    "display_html(\"<b> ANOVA test of Black HTE interaction. </b>\")\n",
    "anova(hte.black, hte.black.null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hte.latino.null = d[ , lm(GotResponse ~ I(ethnic_cue != 0) + pct_latino)]\n",
    "hte.latino      = d[ , lm(GotResponse ~ I(ethnic_cue != 0) * pct_latino)]\n",
    "\n",
    "display_html(\"<b> ANOVA test of Latino HTE interaction. </b>\")\n",
    "anova(hte.latino, hte.latino.null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hte.arab.null = d[ , lm(GotResponse ~ I(ethnic_cue != 0) + pct_arab)]\n",
    "hte.arab      = d[ , lm(GotResponse ~ I(ethnic_cue != 0) * pct_arab)]\n",
    "\n",
    "display_html(\"<b> ANOVA test of Arab HTE interaction. </b>\")\n",
    "anova(hte.arab, hte.arab.null)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block Fixed Effects \n",
    "Does the presence of minority groups in a district associated with differential responsiveness on the part of registrars? To estimate this effect, we fit models that include block fixed effects together with an interaction term for the percent of a geography's population that is composed of the a particular minority class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hte_felm_dist_null_latino = d[ , felm(GotResponse ~ I(ethnic_cue != 0) + pct_latino     | factor(blockID))]\n",
    "hte_felm_dist_null_black  = d[ , felm(GotResponse ~ I(ethnic_cue != 0) + pct_race_black | factor(blockID))]\n",
    "hte_felm_dist_null_arab   = d[ , felm(GotResponse ~ I(ethnic_cue != 0) + pct_arab       | factor(blockID))]\n",
    "hte_felm_dist_latino      = d[ , felm(GotResponse ~ I(ethnic_cue != 0) * pct_latino     | factor(blockID))]\n",
    "hte_felm_dist_black       = d[ , felm(GotResponse ~ I(ethnic_cue != 0) * pct_race_black | factor(blockID))]\n",
    "hte_felm_dist_arab        = d[ , felm(GotResponse ~ I(ethnic_cue != 0) * pct_arab       | factor(blockID))]\n",
    "hte_felm_dist_latino1     = d[ , felm(GotResponse ~ factor(ethnic_cue) * pct_latino     | factor(blockID))]\n",
    "hte_felm_dist_black1      = d[ , felm(GotResponse ~ factor(ethnic_cue) * pct_race_black | factor(blockID))]\n",
    "hte_felm_dist_arab1       = d[ , felm(GotResponse ~ factor(ethnic_cue) * pct_arab       | factor(blockID))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no evidence that district non-white composition has any moderating effect on the discrimination against all minority groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stargazer(hte_felm_dist_latino, hte_felm_dist_black, hte_felm_dist_arab, type= \"text\", \n",
    "          se = list(felm_rses(hte_felm_dist_latino), felm_rses(hte_felm_dist_black), felm_rses(hte_felm_dist_arab)),\n",
    "          covariate.labels = c(\"Minority Cue\", \n",
    "                               \"Percent Latino\", \"Percent Latino * Minority Cue\", \n",
    "                               \"Percent Black\",  \"Percent Black * Minority Cue\", \n",
    "                               \"Percent Arab\",   \"Percent Arab * Minority Cue\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = capture.output( \n",
    "    stargazer(hte_felm_dist_latino, hte_felm_dist_black, hte_felm_dist_arab, \n",
    "          type= \"latex\", \n",
    "          out = \"./districtHTEminorityTable.tex\", \n",
    "          label = \"tab:districtHTE\",\n",
    "          se = list(felm_rses(hte_felm_dist_latino), felm_rses(hte_felm_dist_black), felm_rses(hte_felm_dist_arab)), \n",
    "          covariate.labels = c(\"Minority\", \"Percent Latino\", \"Percent Latino $\\\\times$ Minority\", \n",
    "                               \"Percent Black\", \"Percent Black $\\\\times$ Minority\", \n",
    "                               \"Percent Arab\", \"Percent Arab $\\\\times$ Minority\"), \n",
    "          omit.stat = c(\"F\", \"ser\"), \n",
    "          align = TRUE    \n",
    "          )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is evidence here that the effect of the Latino cue is moderated by the percent of Latino population in a geography. There is no such support for percent of black population living in a geography, nor for percent of arab population living in a geography. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stargazer(hte_felm_dist_latino1, hte_felm_dist_black1, hte_felm_dist_arab1, type = \"text\",\n",
    "          se = list(felm_rses(hte_felm_dist_latino1), felm_rses(hte_felm_dist_black1), felm_rses(hte_felm_dist_arab1)), \n",
    "          covariate.labels = c(\"Latino\", \"Black\", \"Arab\", \n",
    "                               \"Percent Latino\", \n",
    "                               \"Percent Latino * Latino Cue\", \"Percent Latino * Black Cue\", \"Percent Latino * Arab Cue\", \n",
    "                               \"Percent Black\", \n",
    "                               \"Percent Black * Latino Cue\", \"Percent Black * Black Cue\", \"Percent Black * Arab Cue\", \n",
    "                               \"Percent Arab\", \n",
    "                               \"Percent Arab * Latino Cue\", \"Percent Arab * Black Cue\", \"Percent Arab * Arab Cue\"), \n",
    "          omit.stat = c(\"F\", \"ser\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = capture.output(\n",
    "    stargazer(hte_felm_dist_latino1, hte_felm_dist_black1, hte_felm_dist_arab1, \n",
    "          type = \"latex\", \n",
    "          out = \"./districtHTEminorityTable1.tex\", \n",
    "          label = \"tab:districtHTE1\",\n",
    "          se = list(felm_rses(hte_felm_dist_latino1), felm_rses(hte_felm_dist_black1), felm_rses(hte_felm_dist_arab1)), \n",
    "          covariate.labels = c(\"Latino\", \"Black\", \"Arab\", \"Percent Latino\",\n",
    "                               \"Percent Latino $\\\\times$ Latino\", \"Percent Latino $\\\\times$ Black\", \"Percent Latino $\\\\times$ Arab\", \n",
    "                               \"Percent Black\", \n",
    "                               \"Percent Black  $\\\\times$ Latino\", \"Percent Black  $\\\\times$ Black\", \"Percent Black  $\\\\times$ Arab\", \n",
    "                               \"Percent Arab\", \n",
    "                               \"Percent Arab   $\\\\times$ Latino\", \"Percent Arab   $\\\\times$ Black\", \"Percent Arab   $\\\\times$ Arab\"), \n",
    "              add.lines = list(c(\"Block FE\", \"Yes\", \"Yes\", \"Yes\")), \n",
    "          omit.stat = c(\"F\", \"ser\", \"adj.rsq\"), \n",
    "          align = T, \n",
    "          font.size = \"footnotesize\"    \n",
    "          )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the distribution of Arab-American individuals is unique viz the settlement patters of other groups. Whereas there are very few geographies that have either zero Latino or zero Black population, geographies that have zero arab-american settlement are relatively more common. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[ , .(\"Zero Arab American\" = mean(pct_arab == 0, na.rm=T),\n",
    "       \"Zero Black American\" = mean(pct_race_black == 0, na.rm=T),\n",
    "       \"Zero Latino\" = mean(pct_latino == 0, na.rm=T))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because distributions of people were determined pre-treatment, we can in princple condition on this variable. Despite the distribution of peoples, we still have good randomization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d %>% \n",
    "  .[pct_arab > 0, .(\"White Cue\"  = sum(ethnic_cue == 0), \n",
    "                    \"Latino Cue\" = sum(ethnic_cue == 1), \n",
    "                    \"Black Cue\"  = sum(ethnic_cue == 2), \n",
    "                    \"Arab Cue\"   = sum(ethnic_cue == 3))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so, to assess whether the distribution of Arab American individuals conditions the response of local election officials, we perform a rescaling of the `pct_arab` variable. In particular, we create a three-level indicator varaible for the percent of arab population. If there are zero individuals of Arab heritage, we code `median_arab_pop == 0`. Within the geographies that have *some* Arab heritage population, we perform a median split, coding `median_arab_pop == 1` if the geography falls lower than the median split and `median_arab_pop == 2` if the geography falls higher than the median split. \n",
    "\n",
    "Coincedentally, rough 50% of US geographies that we work with have zero Arab heritage population, so the effect of this is re-coding is to create 50% of our boundaries coded 0, and 25% each coded either 1 or 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d %>% \n",
    "  .[ , median_arab_pop := 0] %>% \n",
    "  .[pct_arab > 0, median_arab_pop := I(pct_arab > median(pct_arab)) + 1] %>% \n",
    "  .[ , .(\"Zero Arab\" = mean(median_arab_pop == 0), \n",
    "         \"1-50th Percentile Arab\" = mean(median_arab_pop == 1), \n",
    "         \"51-100 Percentile Arab\" = mean(median_arab_pop == 2))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This rescaling into a three-level variable increases the variance that is coded into this contextual varaible. While earlier findings of a lack of moderating effect on discrimination may have been due to a lack of variance in the interactive varaible, this is not the case after this rescaling. As such, *if there were to be a contextual relationship* between the proportion of Arab Americans residing in a geography and the nature of prejudicial behavior by local election officials -- perhaps due to contact, implicit biases, or other mechanisms -- they should be captured in this comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pct_arab_minority = d[ , felm(GotResponse ~ I(ethnic_cue != 0) * factor(median_arab_pop) | factor(blockID))]\n",
    "pct_arab_felm     = d[ , felm(GotResponse ~ factor(ethnic_cue) * factor(median_arab_pop) | factor(blockID))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although there is some (non-experimental) evidence that the districts that *do* have some Arab population are more likely to repond to a request, there is no evidence to suggest that there is a contextual relationship between treatment and this compositional feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stargazer(pct_arab_minority, pct_arab_felm, type = 'text', \n",
    "          covariate.labels = c(\"Minority Cue\", \"Latino Cue\", \"Black Cue\", \"Arab Cue\", \n",
    "                               \"1-50pct Arab\", \"51-100pct Arab\", \n",
    "                               \"Minority Cue * 1-50pct Arab\", \"Minority Cue * 51-100pct Arab\", \n",
    "                               \"Latino Cue * 1-50pct Arab\", \"Black Cue * 1-50pct Arab\", \"Arab Cue * 1-50pct Arab\", \n",
    "                               \"Latino Cue * 51-100pct Arab\", \"Black Cue * 51-100pct Arab\", \"Arab Cue * 51-100pct Arab\"),\n",
    "          se = list(felm_rses(pct_arab_minority), felm_rses(pct_arab_felm)),\n",
    "          omit.stat = 'ser', \n",
    "          add.lines = list(c(\"Block FE\", \"Yes\", \"Yes\"))\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = capture.output(\n",
    "    stargazer(pct_arab_minority, pct_arab_felm, \n",
    "          type = 'latex', \n",
    "          out = \"./arabPopRescale.tex\", \n",
    "          covariate.labels = c(\"Minority Cue\", \"Latino Cue\", \"Black Cue\", \"Arab Cue\", \n",
    "                               \"1-50pct Arab\", \"51-100pct Arab\", \n",
    "                               \"Minority Cue * 1-50pct Arab\", \"Minority Cue * 51-100pct Arab\", \n",
    "                               \"Latino Cue * 1-50pct Arab\", \"Black Cue * 1-50pct Arab\", \"Arab Cue * 1-50pct Arab\", \n",
    "                               \"Latino Cue * 51-100pct Arab\", \"Black Cue * 51-100pct Arab\", \"Arab Cue * 51-100pct Arab\"),\n",
    "          se = list(felm_rses(pct_arab_minority), felm_rses(pct_arab_felm)),\n",
    "          omit.stat = 'ser',\n",
    "          header = FALSE,\n",
    "          label = 'tab:arabRescale', \n",
    "          font.size = \"footnotesize\", \n",
    "          align = TRUE, \n",
    "          add.lines = list(c(\"Block FE\", \"Yes\", \"Yes\"))    \n",
    "         )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.B Political Characteristics \n",
    "<a id='politics'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We now turn to examine whether political features shape the responsiveness of local election officials to request for information from our emails. One possiblity for why this might be the case is that local election officials may be politically responsive to the constituents in their geographies. That is, they may behave in a way that is line with the voters in their geographies; rather than, for example, the state law that specifies how the election should be administered. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Political Motivations for Discrimination \n",
    "If it were the case that there were political motivations that were leading to discrimination, then we might expect that local elections officials whose jurisdictions voted more Republican in (a) 2012 or (b) 2016 were more likely to provide less service to minority requesters. Importantly, figuring out this difference involves distinguishing between the *general* increase in responsiveness that exists in the south. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tony McGovern Data \n",
    "This electoral data comes from Tony McGovern, who scraped the data from townhall.com to produce the data. Many thanks to Tony for the work.  There is just a little bit of strangeness has to be dealt with in this data. That strangeness is that the leading zeros in the `combined_fips` field have been dropped. This means that for Alabama, Alaska, Arizon,a Arkansas, California, Colorado and Connecticut we're not matching FIPS. No matter, we'll just correct that. \n",
    "\n",
    "We have one county that is not matching. The Oglala Lakota county in SD. The rest seem to be matching. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results2016 <- fread('../data/County_Level_Election_Results_12-16/2016_US_County_Level_Presidential_Results.csv', \n",
    "                     colClasses=c(combined_fips=\"character\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the `clinton_voteshare` and `trump_voteshare` two-party vote-share indicators. These are just \n",
    "\\begin{align}\n",
    "clinton\\_voteshare &= \\frac{votes\\_dem}{votes\\_gop + votes\\_dem} \\\\ \n",
    "trump\\_voteshare &= \\frac{votes\\_gop}{votes\\_gop + votes\\_dem}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results2016 = results2016[ , ':='(clinton_voteshare = votes_dem / (votes_gop + votes_dem),\n",
    "                    trump_voteshare   = votes_gop / (votes_gop + votes_dem) )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "problemFIPS <- c(\"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\")\n",
    "\n",
    "results2016 = results2016[state_abbr %in% problemFIPS, \n",
    "            ':='(STATEFP  = paste0(\"0\", substr(x = combined_fips, start=1, stop=1)), \n",
    "                 COUNTYFP = substr(x = combined_fips, start = 2, stop = 4) )] \n",
    "results2016 = results2016[!(state_abbr %in% problemFIPS), \n",
    "            ':='(STATEFP  = substr(x = combined_fips, start = 1, stop = 2), \n",
    "                 COUNTYFP = substr(x = combined_fips, start = 3, stop = 5) )]\n",
    "results2016 = results2016[ , ':='(STATEFP  = as.character(STATEFP), \n",
    "                    COUNTYFP = as.character(COUNTYFP))]\n",
    "\n",
    "d = d[ , ':='(STATEFP  = as.character(STATEFP), \n",
    "          COUNTYFP = as.character(COUNTYFP))]\n",
    "d = d[st %in% problemFIPS, \n",
    " ':='(STATEFP = paste0(\"0\", STATEFP))]\n",
    "d = d[nchar(COUNTYFP)==1, COUNTYFP := paste0(\"00\", COUNTYFP)]\n",
    "d = d[nchar(COUNTYFP)==2, COUNTYFP := paste0(\"0\", COUNTYFP)]\n",
    "\n",
    "results2016 = results2016[ , merge_id_results2016 := 4]\n",
    "d = d[ , merge_id_d := 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = merge(d, results2016, all.x = TRUE, \n",
    "             by.x = c(\"STATEFP\", \"COUNTYFP\"), by.y = c(\"STATEFP\", \"COUNTYFP\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Romney Vote Share \n",
    "Here, we create a variable called `romney_voteshare` that is the percent of the voteshare won by the Republican candidate in the 2012 Presidential election. It is a simple transformation of the `obama_voteshare` variable that we used to block. We make the transformation only for comparability to the `trump_voteshare` variable that we will report in the next section. \n",
    "\\begin{equation} \n",
    "romney\\_voteshare = .5 - \\frac{obama\\_margin}{2}\n",
    "\\end{equation} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = d[ , romney_voteshare := .5 - (obama_margin / 2)]\n",
    "source(\"http://ischool.berkeley.edu/~d.alex.hughes/code/pubPlot.R\")\n",
    "d[ , plot(romney_voteshare, trump_voteshare, pch = \".\", \n",
    "          xlim=c(0,1), ylim=c(0,1))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Models on 2012 Returns \n",
    "One of the blocking features that we used in the design of this experiment was the 2012 two-party vote share. As a result, we have very good, orthagonal variation on the percent of the elctoral that voted for a republican vs. democratic candidate and the treatment conditions that we asisgned to each local election offical. There, is, however, no evidence to suggest that differences in the 2012 two-party vote share change the responsiveness of local election officials to our stimulus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m0  = d[ , lm(GotResponse ~ factor(ethnic_cue) * romney_voteshare)]\n",
    "m0a = d[ , felm(GotResponse ~ factor(ethnic_cue) * romney_voteshare | factor(blockID))]\n",
    "stargazer(m0, m0a, type = \"text\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Models on 2016 Returns \n",
    "We turn to examine whether the relationship between our stimulus and the outcome is mediated by the 2016 electoral outcomes. Here, we fit models that interact the `ethic_cue` treatment variable with Republican two-party voteshare in the election (called `trump_voteshare`). We find that, in models that *do* or *do not* include block indicators that there is a clear effect: geograhies that voted at higher rates for the Republican candidate also discriminate against Arab names at higher rates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = d[ , lm(GotResponse ~   factor(ethnic_cue) * trump_voteshare)]\n",
    "m2 = d[ , felm(GotResponse ~ factor(ethnic_cue))]\n",
    "m3 = d[ , felm(GotResponse ~ factor(ethnic_cue) * trump_voteshare | factor(blockID))]\n",
    "m4 = d[ , felm(GotResponse ~ I(ethnic_cue != 0) * trump_voteshare | factor(blockID))]\n",
    "stargazer(m0, m0a, m1, m3, type = 'text', omit.stat = c('ser', 'F'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = capture.output( \n",
    "    stargazer(m0, m0a, m1, m3, \n",
    "              type = 'latex', \n",
    "              out = './trumpHTEtable.tex', \n",
    "              label = 'tab:trumpHTE', \n",
    "              covariate.labels = c('Latino Cue', 'Black Cue', 'Arab Cue', '2012 R. Vote Share', \n",
    "                                   'Latino Cue $\\\\times$ 2012 R. Votes Share',\n",
    "                                   'Black Cue $\\\\times$ 2012 R. Vote Share', \n",
    "                                   'Arab Cue $\\\\times$ 2012 R. Vote Share', \n",
    "                                   '2016 R. Vote Share', \n",
    "                                   'Latino Cue $\\\\times$ 2016 R. Votes Share',\n",
    "                                   'Black Cue $\\\\times$ 2016 R. Vote Share', \n",
    "                                   'Arab Cue $\\\\times$ 2016 R. Vote Share', \n",
    "                                   'Intercept'), \n",
    "              add.lines = list(c('Block Fixed Effects', 'No', 'Yes', 'No', 'Yes')),\n",
    "              omit.stat = c('ser', 'F', 'adj.rsq'), \n",
    "              font.size = \"small\"\n",
    "             )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred.data = data.frame( \n",
    "  ethnic_cue = factor(c(rep(0, 100), rep(1, 100), rep(2, 100), rep(3, 100))), \n",
    "  trump_voteshare = rep(seq(from=min(d$trump_voteshare, na.rm=T), to=max(d$trump_voteshare, na.rm=T), length.out = 100) , 4)\n",
    ")\n",
    "pred.data$fit    = predict.lm(m1, newdata=pred.data, interval = 'confidence')\n",
    "pred.data$se.fit = predict.lm(m1, newdata=pred.data, se.fit = T,)$se.fit\n",
    "pred.data$low.ci = predict.lm(m1, newdata = pred.data, interval = 'confidence', level=0.95)[ , 'lwr']\n",
    "pred.data$high.ci= predict.lm(m1, newdata = pred.data, interval = 'confidence', level=0.95)[ , 'upr']\n",
    "pred.data = data.table(pred.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "white.col  = rgb(.7, .7, .7, 0.5, 255)\n",
    "latino.col = rgb(0, 0, .7, 0.25)\n",
    "black.col  = rgb(0, 0, 0.3, 0.25)\n",
    "arab.col   = rgb(.7, .7, .7, 0.75)\n",
    "dens = density(d$trump_voteshare, na.rm=T, from = 0, to=1)\n",
    "\n",
    "pdf('./HTEtrump.pdf')\n",
    "source(\"http://ischool.berkeley.edu/~d.alex.hughes/code/pubPlot.R\")\n",
    "pred.data[ethnic_cue == 0, \n",
    "          plot(x = trump_voteshare, y = fit, \n",
    "               type = \"n\", ylim = c(0.3, 0.8), col = \"black\", \n",
    "               ylab = \"Probability Respond\",   yaxt = 'n', \n",
    "               xlab = \"Republican Vote Share\", xaxt = 'n',\n",
    "#                main = \"Republican Districts Less Responsive to Arab Names\"\n",
    "              )]\n",
    "axis(1, at = c(seq(0,.8, 0.2), .95), labels = c(\"0\", \"20\", \"40\", \"60\", \"80\", \"100%\"))\n",
    "axis(2, at = seq(0.2, 0.8, by=0.1), labels = c(\"20\", \"30\", \"40\", \"50\", \"60\", \"70\", \"80%\"))\n",
    "pred.data[ethnic_cue == 0, \n",
    "          polygon(x = c(trump_voteshare, rev(trump_voteshare)), \n",
    "                  y = c(low.ci, rev(high.ci)), \n",
    "                  border = NA, \n",
    "                  col = white.col)]\n",
    "pred.data[ethnic_cue == 3, \n",
    "          polygon(x = c(trump_voteshare, rev(trump_voteshare)), \n",
    "                  y = c(low.ci, rev(high.ci)), \n",
    "                  border = NA, col = arab.col)]\n",
    "pred.data[ethnic_cue == 0, \n",
    "          points(x = trump_voteshare, y = fit,\n",
    "                 type = \"l\", lty = 1)]\n",
    "pred.data[ethnic_cue == 3, \n",
    "          points(x = trump_voteshare, \n",
    "                 y = fit, \n",
    "                 type = \"l\", lty = 2)]\n",
    "legend(x=.15, y=.45, legend = c(\"White Cue\", \"Arab Cue\"), border = NA, \n",
    "#        fill = c(white.col, arab.col), \n",
    "       lty = c(1,2), merge = TRUE,\n",
    "       bty='n', cex = 1.5)\n",
    "# legend(x=.15, y=.40, legend = c(\"White Cue\", \"Arab Cue\"), border = NA, \n",
    "#        lty = c(1,2), \n",
    "#        bty='n', cex = 1.5)\n",
    "\n",
    "\n",
    "rug(d$trump_voteshare)\n",
    "points(x = dens$x, y=.3 + dens$y/50, type = 'l') # moving into the plotting window \n",
    "dev.off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./HTEtrump.pdf'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models Fit on 2012 v. 2016 Republican Vote Share \n",
    "Is there something unique about areas that moved _uniquely_ toward Trump? That is, given a baseline level of support for the Republican candidate in 2012, did those that moved toward the Republican candidate in 2016 present a unique effect? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m0  = d[ ,   lm(GotResponse ~ factor(ethnic_cue) * I(trump_voteshare - romney_voteshare) +  romney_voteshare)]\n",
    "m1  = d[ , felm(GotResponse ~ factor(ethnic_cue) * I(trump_voteshare - romney_voteshare) +  romney_voteshare | blockID)]\n",
    "stargazer(m0, m1, type = \"text\", \n",
    "    covariate.labels = c(\"Latino\", \"Black\", \"Arab\", \"R Diff\", \"Romney\", \n",
    "                         \"Latino * R Diff\", \"Black * R Diff\", \"Arab * R Diff\")\n",
    "          )\n",
    "\n",
    "d[ , plot(I(trump_voteshare - romney_voteshare), log(pct_arab+0.0001), pch = \".\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"interruption\"></a> \n",
    "# 4. Interruption by Notification\n",
    "[toc](#toc)\n",
    "\n",
    "As we note in the paper, on November 2nd, an email was sent from the NASS, the professional organization for secrataaries of state. While this event presents a potential problem of interference between units, the responses collected from units *before* the interference event can reasonably be used to draw inference. \n",
    "\n",
    "However, at the same time, the nature of the data generating procedure -- an audit study where the failure to obtain a response from an experiemntal unit is a meaningful piece of data -- may raise some questions about how the notification of secretaries of state affected the results of this audit study. We make three claimss. \n",
    "\n",
    "1. In this study, the time at which widespread notification was sent out left little data to *potentially* effect causal effect estimates. \n",
    "2. Responses collected before widespread notification are meaningful data points; \n",
    "3. Causal estimates will only be biased if notification causes conditionally different response to treatment. \n",
    "\n",
    "First and foremost, in this study as well as the study conduced by White, Nathan, and Faller (2015) as well as several other audit studies (e.g. Gell-Redman (*in press*), Crabtree (*in press*)) the vast majority of those responses that are going to be returned to the sender by subjects are received within 27 hours of sending. We note this because there is a clear pattern that subjects fall into. (1) They respond immediately; (2) they respond at the beginning of business the next day; or (3) they do not respond. Indeed, registrars in our sample fall well within this pattern of behavior -- the median time to response, for those who respond to us as 8 hours. \n",
    "\n",
    "In the remainder of this section, we demonstrate how slow the response rate was in the time immediately preceeding the widespread notification. \n",
    "\n",
    "White, Nathan and Faller (2015) elect to remove from consideration states that the authors suspected had violated the non-interference or exclusion restriction assumptions in their audit study. We contend that such a determination is unnecessairily consaervative. While it is true that responses that are received after the widespread notification of the study are indeed unique responses, those responses that are received before this notificaiton adhere to all the assumptions necessary to elucidate causal quantities from this research design. \n",
    "\n",
    "To demonstrate that there is little effect of trimming our data to the point that widespread notification was sent to Registrars, in this section we employ a series of duration models. We have not chosen to report our primary causal effects through these models because the difference in means and OLS estimates we *do* report remain unbiased estimators and are considerably simpler to conceptualize. Nonetheless, in this section we demonstrate that moving the censoring point for *non-response* from election day to the time when notification was sent to registrars has no appreciable effect on causal estimates.\n",
    "\n",
    "Second, the *ex ante* expectation for a widespread notification is that registrars will be less likely to respond to an email originating from our sending domain. But, there is little reason to suspect that registrars should behave in this way. We test for differential responsivness pre-and post mailer, and find no evidence to support a theory of differential responsiveness to treatment conditional on this split. \n",
    "\n",
    "We ran pilots in MN, CA, OR, WA, NV. While the timing that we report in the main text excludes these western statea and Minnesota (which was also not utilized by White, Nathan, and Faller (2015)), the results that we estimate are not unique to this composed set of states. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"rateOfResponse\"></a> \n",
    "## 4.A Rate of Responses\n",
    "[toc](#toc)\n",
    "\n",
    "As a descriptive matter, the rate of email responses from registrars had slowed to little more than a trickle at the time that the NASS sent an email to its members. In the next figure, we plot the total number of emails received back to our servers on the y-axis of the plot, and the (logged) time since sending on the x-axis. The rate of response of registrars might be considered as the first derivative of this plot -- or simply understood as the rate of change at a particular time within the study window. \n",
    "\n",
    "As the figure shows, the vast majority of response received by our servers were received quickly after leaving. The median time to response is just over 8 hours, though this distribution is highly right skewed such that a very small number of registrars take a very long time to response (e.g. $\\mu$ time is 14 hours, while the $90^{th}$ percentile time to response is only 11.2 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(interrupt) { \n",
    "censoring.date = \"2016-11-08 00:00:00\"\n",
    "clerk.email.date = \"2016-11-02 20:00:00\" # NASS email to all orgs\n",
    "send.date = \"2016-10-31 09:00:00\"\n",
    "sd = create.survival.data(cd = censoring.date)\n",
    "\n",
    "rm(d)\n",
    "d = load.data()\n",
    "\n",
    "\n",
    "## also make data on non-surivival set for estimating interaction\n",
    "nineOclock = as.numeric(as.POSIXct(\"2016-11-02 09:00:00\"))\n",
    "oneOclock  = as.numeric(as.POSIXct(\"2016-11-02 13:00:00\"))\n",
    "\n",
    "d = d[ReplyDate != \"\", numReply := as.numeric(as.POSIXct(ReplyDate))]\n",
    "d = d[ , afterNASS := 0]\n",
    "d = d[ReplyDate != \"\"] %>% \n",
    "  .[as.numeric(as.POSIXct(ReplyDate)) > as.numeric(as.POSIXct(clerk.email.date)), \n",
    "      afterNASS := 1]\n",
    "d[ , table(afterNASS)]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What was the median time to response, for those responses that we received? Note that this is *not* the time that local election officials use to respond to our stimulus, but rather is the time between when we *sent* the email and when the election official responded. We may have sent the email at 10:00a local time, but the registrar may not have opened the email until after lunch. These two-plus hours are included in the \"time to response\" metric that we report here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(interrupt) { \n",
    "x = sd[GotResponse == 1 , quantile(x = time, probs = c(.5, .75, .9), na.rm = TRUE)] / 3600 \n",
    "\n",
    "display_html(sprintf(\"The median time to response was %.1f hours.\", x[1]))\n",
    "display_html(sprintf(\"The 75th percentile time to response was %.1f hours.\", x[2]))\n",
    "display_html(sprintf(\"The 90th percentile time to response was %.1f hours.\", x[3]))\n",
    "\n",
    "x = sd[GotResponse == 1 , mean(x = time, na.rm = TRUE)] / 3600 \n",
    "\n",
    "display_html(sprintf(\"The mean time to response was %.1f hours.\", x))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(interrupt) { \n",
    "pdf(\"./rateOfResponse.pdf\")\n",
    "source(\"http://ischool.berkeley.edu/~d.alex.hughes/code/pubPlot.R\")\n",
    "par(oma=c(0,0,0,0), mar = c(5,7,4,2)+.1, lwd=3)\n",
    "sd[GotResponse == 1] %>% \n",
    "#sd[GotResponse == 1 & !(state %in% c(\"Oregon\", \"Minnesota\", \"California\", \"Oregon\", \"Washington\"))] %>% \n",
    "  .[order(time), .(time = time, count = 1:.N)] %>% \n",
    "  .[ , plot(x=time, y=count, type = \"l\", log = \"x\", \n",
    "           main = \"All Responses\", \n",
    "           xaxt = 'n', xlab = \"Days Since Sending\",\n",
    "           yaxt = 'n', ylab = \"\", ylim = c(0,4000))\n",
    "   ]\n",
    "title(ylab=\"Responses\", line=5)\n",
    "\n",
    "# axis(side = 1, at = c(25200, 1:7*(100800)), labels = paste0(0:7))\n",
    "axis(side = 1, at = c(25200, 1:8*(86400)+25200), labels = paste0(0:8))\n",
    "axis(side = 2, at = c(0, 1000, 2000, 3000, 4000), labels = c(\"0\", \"1,000\", \"2,000\", \"3,000\", \"4,000\"))\n",
    "abline(v = 25200 + (8*86400),\n",
    "       lty = 2, col = \"gray50\")\n",
    "abline(v = 25200 + (2*86400) + 3600 * 20,\n",
    "       lty = 2, col = \"gray50\")\n",
    "text(x=6.3*100800, y=400, labels=\"Election Day\", srt=90)\n",
    "text(x=2.4*100800, y=400, labels=\"NASS Email\", srt=90)\n",
    "text(x=.35 * 100800, y = 3750, labels=\"(a)\", cex = 2, col = \"grey50\", font=2)\n",
    "\n",
    "dev.off()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./rateOfResponse.pdf\" alt=\"Experiment Overview\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(interrupt) { \n",
    "pdf(\"./rateOfResponsePerCondition.pdf\")\n",
    "source(\"http://ischool.berkeley.edu/~d.alex.hughes/code/pubPlot.R\")\n",
    "par(oma=c(0,0,0,0), mar = c(5,7,4,2)+.1, lwd=3)\n",
    "sd[GotResponse == 1 & ethnic_cue == 0 & !(state %in% c(\"Oregon\", \"Minnesota\", \"California\", \"Oregon\", \"Washington\", \"Michigan\", \"New Hampshire\", \"Colorado\"))] %>% \n",
    "  .[order(time), .(time = time, count = 1:.N)] %>% \n",
    "  .[ , plot(x=time, y=count, type = \"l\", lty = 1, log = \"x\", col = \"grey\", lwd = 5,\n",
    "           main = \"No Interference States:\\n Responses by Condition\", \n",
    "           xaxt = 'n', xlab = \"Days Since Sending\",\n",
    "           yaxt = 'n', ylab = \"\", ylim = c(0,1000))]\n",
    "\n",
    "sd[GotResponse == 1 & ethnic_cue == 1 & !(state %in% c(\"Oregon\", \"Minnesota\", \"California\", \"Oregon\", \"Washington\", \"Michigan\", \"New Hampshire\", \"Colorado\"))] %>% \n",
    "  .[order(time), .(time = time, count = 1:.N)] %>% \n",
    "  .[ , lines(x=time, y=count, lty = 4)]\n",
    "sd[GotResponse == 1 & ethnic_cue == 2 & !(state %in% c(\"Oregon\", \"Minnesota\", \"California\", \"Oregon\", \"Washington\", \"Michigan\", \"New Hampshire\", \"Colorado\"))] %>% \n",
    "  .[order(time), .(time = time, count = 1:.N)] %>% \n",
    "  .[ , lines(x=time, y=count, lty = 3)]\n",
    "sd[GotResponse == 1 & ethnic_cue == 3 & !(state %in% c(\"Oregon\", \"Minnesota\", \"California\", \"Oregon\", \"Washington\", \"Michigan\", \"New Hampshire\", \"Colorado\"))] %>% \n",
    "  .[order(time), .(time = time, count = 1:.N)] %>% \n",
    "  .[ , lines(x=time, y=count, lty = 2)]\n",
    "       \n",
    "title(ylab=\"Responses\", line=4)\n",
    "axis(side = 1, at = c(25200, 1:8*(86400)+25200), labels = paste0(0:8))\n",
    "axis(side = 2, at = c(0, 250, 500, 750, 1000), labels = c(\"0\", \"250\", \"500\", \"750\", \"1,000\"))\n",
    "abline(v = 25200 + (8*86400),\n",
    "       lty = 2, col = \"gray50\")\n",
    "abline(v = 25200 + (2*86400) + 3600 * 20,\n",
    "       lty = 2, col = \"gray50\")\n",
    "text(x=6.3*100800, y=100, labels=\"Election Day\", srt=90)\n",
    "text(x=2.4*100800, y=100, labels=\"NASS Email\", srt=90)\n",
    "text(x=.35 * 100800, y = 950, labels=\"(c)\", cex = 2, col = \"grey50\", font=2)\n",
    "legend(x = 43200, y = 350, \n",
    "       legend = c(\"White\", \"Black\", \"Latino\", \"Arab\"), cex=1.5,\n",
    "       lty = c(1, 3, 4, 2), lwd = c(5,3,3,3), \n",
    "       col = c(\"grey\", \"black\", \"black\", \"black\"))\n",
    "dev.off()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./rateOfResponsePerCondition.pdf\" alt=\"Experiment Overview\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstrate that using all the states (that is, including MN and the western states does not change the estimates that we report. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(interrupt) { \n",
    "pdf(\"./rateOfResponsePerConditionAllStates.pdf\")\n",
    "source(\"http://ischool.berkeley.edu/~d.alex.hughes/code/pubPlot.R\")\n",
    "par(oma=c(0,0,0,0), mar = c(5,7,4,2)+.1, lwd=3)\n",
    "sd[GotResponse == 1 & ethnic_cue == 0] %>% \n",
    "  .[order(time), .(time = time, count = 1:.N)] %>% \n",
    "  .[ , plot(x=time, y=count, type = \"l\", log = \"x\", \n",
    "           main = \"All States: Responses by Condition\", \n",
    "           xaxt = 'n', xlab = \"Days Since Sending\",\n",
    "           yaxt = 'n', ylab = \"\", ylim = c(0,1000), \n",
    "           lty = 1, col = \"grey\", lwd = 5)\n",
    "   ]\n",
    "# for(i in 1:3) { \n",
    "#     sd[GotResponse == 1 & ethnic_cue == i] %>% \n",
    "#       .[order(time), .(time = time, count = 1:.N)] %>% \n",
    "#       .[ , lines(x=time, y=count, lty = i+1)]\n",
    "# }\n",
    "sd[GotResponse == 1 & ethnic_cue == 1] %>% \n",
    "  .[order(time), .(time = time, count = 1:.N)] %>% \n",
    "  .[ , lines(x=time, y=count, lty = 4)]\n",
    "sd[GotResponse == 1 & ethnic_cue == 2] %>% \n",
    "  .[order(time), .(time = time, count = 1:.N)] %>% \n",
    "  .[ , lines(x=time, y=count, lty = 3)]\n",
    "sd[GotResponse == 1 & ethnic_cue == 3] %>% \n",
    "  .[order(time), .(time = time, count = 1:.N)] %>% \n",
    "  .[ , lines(x=time, y=count, lty = 2)]    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "title(ylab=\"Responses\", line=4)\n",
    "axis(side = 1, at = c(25000, 1:7*(100800)), labels = paste0(0:7))\n",
    "axis(side = 2, at = c(0, 250, 500, 750, 1000), labels = c(\"0\", \"250\", \"500\", \"750\", \"1,000\"))\n",
    "abline(v = 25200 + (2*86400) + 3600 * 20,\n",
    "       lty = 2, col = \"gray50\")\n",
    "abline(v = as.numeric(\n",
    "           as.POSIXct(censoring.date) - sd[ , mean(Date, na.rm = TRUE)]),\n",
    "       lty = 2, col = \"gray50\")\n",
    "text(x=6*100800, y=100, labels=\"Election Day\", srt=90)\n",
    "text(x=2.4*100800, y=100, labels=\"NASS Email\", srt=90)\n",
    "text(x=.35 * 100800, y = 950, labels=\"(b)\", cex = 2, col = \"grey50\", font=2)\n",
    "legend(x = 43200, y = 350, \n",
    "       lty = c(1,3,4,2), lwd = c(5,3,3,3), \n",
    "       legend = c(\"White\", \"Black\", \"Latino\", \"Arab\"), \n",
    "       col = c(\"grey\", \"black\", \"black\", \"black\"), , cex=1.5)\n",
    "\n",
    "dev.off()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./rateOfResponsePerConditionAllStates.pdf\" alt=\"Experiment Overview\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"timeToRespond\"></a>\n",
    "## 4.B Time for Registrars to Respond \n",
    "[toc](#toc)\n",
    "\n",
    "An interesting question that has not been answered in audit studies conducted with government officicals is the amount of time that the audit study requires of each official. This question has its roots both in understanding what is being asked of subjects, and also in understanding the costs of executing the audit study. \n",
    "\n",
    "One of the difficulties with this measurement -- a difficulty that has likely caused previous scholars to overlook this interesting result -- is that it is not possible using standard consumer email to know *when* a subject began the task of addressing the audit. One might consider the response task beginning at the time the audit was initiated and then simply compute the time between receiving a response to the audit task. Clearly, however, this will tend to over-estimate the amount of time required to a task, beacuse it is not the case that *all* units begin working on the task immediately. \n",
    "\n",
    "In our project, we utilized a standard practice in direct-consumer contact and included an image in our email. This image, which was a `1x1` pixel white image at the bottom of our email -- an image commonly called a \"tracking pixel\" loaded from our sending server at the time that the subject opened the email. This provides us with an approximate time that our units began the task, permitting us to calculate a difference between the beginning and completion of the task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tracker = fread('../mailer/tracked_hits.csv')\n",
    "\n",
    "tracker = merge(tracker, d[ , .(registrar_id, RecvDate, ReplyDate, ethnic_cue, GotResponse, blockID)], \n",
    "               by.x = \"registrar_id\", by.y = \"registrar_id\")\n",
    "\n",
    "tracker = tracker[ , timestamp.d := ymd_hms(timestamp)]\n",
    "tracker = tracker[ , RecvDate.d := ymd_hms(RecvDate)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tracker = tracker[ , seconds.d := RecvDate.d - timestamp.d]\n",
    "tracker = tracker[ , seconds   := as.numeric(seconds.d)]\n",
    "tracker = tracker[ , minutes   := seconds / 60] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we take only the first tracker hit for each LEO. This computes a conservative (long) estimate of the time to complete. We note at this point that we drop *negative* times to complete. These negative times to complete occur because of image caching, mostly on users who are utilizing consumer email clients. We note that LEOs who are using this form of technology are non-random. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tracker = tracker[ , numHits := .N, by = registrar_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker %>% \n",
    "  .[ , .(min_seconds = min(seconds, na.rm = T), \n",
    "         max_seconds = max(seconds, na.rm = T), \n",
    "         ethnic_cue  = mean(ethnic_cue, na.rm = T)), \n",
    "    keyby = .(registrar_id)] %>% \n",
    "  .[min_seconds > 0 & max_seconds > 0] %>% \n",
    "  .[ , .(\"Median Mininum Seconds\" = median(min_seconds), \n",
    "         \"Median Mininum Minutes\" = as.numeric(median(min_seconds) / 60), \n",
    "         \"Median Maximum Seconds\" = median(max_seconds), \n",
    "         \"Median Maximum Minutes\" = as.numeric(median(max_seconds) / 60))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf(file = './timeToResponse.pdf')\n",
    "source(\"http://ischool.berkeley.edu/~d.alex.hughes/code/pubPlot.R\")\n",
    "tracker %>% \n",
    "   .[ , .(min_seconds = min(seconds, na.rm = T), \n",
    "          max_seconds = max(seconds, na.rm = T), \n",
    "          ethnic_cue  = mean(ethnic_cue, na.rm = T)), \n",
    "    keyby = .(registrar_id)] %>% \n",
    "  .[min_seconds > 0 & max_seconds > 0] %>% \n",
    "  .[order(min_seconds)] %>% \n",
    "  .[ , .(t = as.numeric(min_seconds) / 60, count = 1:.N)] %>% \n",
    "  .[ , plot(t, count, type = \"l\", \n",
    "            xlab = \"Minutes Between Open and Response\", xlim = c(0, 120), \n",
    "            ylab = \"Number of Hits\", ylim = c(0, 2155)\n",
    "           )]\n",
    "dev.off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"./timeToResponse.pdf\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"noInterruptionDifference\"></a>\n",
    "\n",
    "## 4.C No Difference in Estimated Effect Due to Email \n",
    "[toc](#toc)\n",
    "\n",
    "In duration models, the researcher specifies the point at which a \"failure to convert\" is recorded as such a failure. This class of models, developed in the context of health studies provide unbiased estimates in the face of right censoring. \n",
    "\n",
    "In the design that we registered with EGAP, we specified the censoring date as election day. On election day, any registrar who had not responded to our mailer would be classified as a non-response. This is the date that we use for the main analysis, but we also demonstrate that changing the censoring date does not change the inference drawn from this experiment. In particular, we demonstrate that changing the censoring date to the time of the NASS email to registrars' state organizations neither changes the point estimate of the causal effect, nor does it change the inference of the causal effect of senders' names. \n",
    "\n",
    "To perform this check, we create two survival data sets -- one with the censoring date on election day, and the other with the censoring date at the time of the NASS emailer. On both sets of data, we estiamte Cox Proportional Hazards models with only a term for the `ethnic_cue` sent to registrars. \n",
    "\n",
    "As is evident in the plot, and borne out in the estimated models, the estimates of the causal effect are stable both in terms of location and uncertainty in the models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(interrupt) { \n",
    "sd1 = create.survival.data(cd = censoring.date)\n",
    "sd2 = create.survival.data(cd = clerk.email.date)\n",
    "\n",
    "mc1 = sd1[!(state %in% c(\"Oregon\", \"Minnesota\", \"California\", \"Oregon\", \"Washington\", \"Michigan\", \"New Hampshire\", \"Colorado\")), \n",
    "       coxph(Surv(time, survRespond) ~ 1 + I(ethnic_cue != 0))]\n",
    "mc2 = sd2[!(state %in% c(\"Oregon\", \"Minnesota\", \"California\", \"Oregon\", \"Washington\", \"Michigan\", \"New Hampshire\", \"Colorado\")), \n",
    "       coxph(Surv(time, survRespond) ~ 1 + I(ethnic_cue != 0))]\n",
    "mc3 = sd1[ , coxph(Surv(time, survRespond) ~ 1 + I(ethnic_cue != 0))]\n",
    "mc4 = sd2[ , coxph(Surv(time, survRespond) ~ 1 + I(ethnic_cue != 0))]\n",
    "\n",
    "stargazer(mc1, mc2, mc3, mc4, type = \"text\", \n",
    "          add.lines = list(c(\"Censoring Date\", \"Election\", \n",
    "                             \"NASS Email\", \"Election\", \"NASS Email\")))\n",
    "    } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(interrupt) { \n",
    "fc1 = sd1[!(state %in% c(\"Oregon\", \"Minnesota\", \"California\", \"Oregon\", \"Washington\", \"Michigan\", \"New Hampshire\", \"Colorado\")), \n",
    "       coxph(Surv(time, survRespond) ~ 1 + factor(ethnic_cue))]\n",
    "fc2 = sd2[!(state %in% c(\"Oregon\", \"Minnesota\", \"California\", \"Oregon\", \"Washington\", \"Michigan\", \"New Hampshire\", \"Colorado\")), \n",
    "       coxph(Surv(time, survRespond) ~ 1 + factor(ethnic_cue))]\n",
    "fc3 = sd1[ , coxph(Surv(time, survRespond) ~ 1 + factor(ethnic_cue))]\n",
    "fc4 = sd2[ , coxph(Surv(time, survRespond) ~ 1 + factor(ethnic_cue))]\n",
    "\n",
    "stargazer(fc1, fc2, fc3, fc4, type = \"text\", \n",
    "          add.lines = list(c(\"Censoring Date\", \"Election\", \n",
    "                             \"NASS Email\", \"Election\", \"NASS Email\")))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if(interrupt) { \n",
    "m = capture.output( \n",
    "    stargazer(mc1, mc2, mc3, mc4, fc1, fc2, fc3, fc4, \n",
    "          type = \"latex\",\n",
    "          out = \"./survivalModels.tex\",\n",
    "          add.lines = list(c(\"Data Subset\", \"Clean\", \"Clean\", \"All\", \"All\", \"Clean\", \"Clean\", \"All\", \"All\"),\n",
    "                           c(\"Censoring Date\", \"Election\", \"Clerk\", \"Election\", \"Clerk\", \"Election\", \"Clerk\", \"Election\", \"Clerk\")), \n",
    "          digits = 2, \n",
    "          omit.stat = c(\"wald\", \"lr\", \"max.rsq\", \"ll\", \"logrank\"), \n",
    "          column.sep.width = \"0pt\", \n",
    "          covariate.labels = c(\"Minority Cue\", \"Latino Cue\", \"Black Cue\", \"Arab Cue\"),\n",
    "          dep.var.caption = NULL, \n",
    "          dep.var.labels.include = FALSE\n",
    "         )\n",
    "    )\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that even for local election officials who take more than 5 minutes, it is possible that they have opened the email to read the contents, and then set aside time to respond at a later time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_html(\"To toggle on/off the raw code, click <a href='javascript:code_toggle()'>here</a>.\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
